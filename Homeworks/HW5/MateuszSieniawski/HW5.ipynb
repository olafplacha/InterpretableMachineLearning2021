{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "### Author: Mateusz Sieniawski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was working on housing data (https://www.kaggle.com/shivachandel/kc-house-data). I've trained three models and calculated their permutational variable importance for each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "The most important variable for XGBoost is `latitute`, and the second most important is `grade` of the house. These results are consistent with its SHAP attributions.\n",
    "\n",
    "![xgboost](img/xgboost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "\n",
    "Random forest behaves very differently in comparison with the XGBoost model. It uses mostly only `latitute`, and a little bit of `longitude` and `grade` in its predictions. On the other hand, XGBoost used many more variables. In random forest variables other than `latitude`, `longitude`, and `grade` have almost no impact on the prediction. It might suggest that this model is overfitted to `latitude`. It might be a good idea to change `max_features` hyperparameter to prevent this behaviour. \n",
    "\n",
    "![rf](img/rf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "I was very suprised when I looked at the variable importance of linear regression an initially though that there is a bug in my code. However, the linear regression was the worst model of these 3. XGBoost and Random Forest had mean squared log error around 0,03, whilst the linear regression loss was 0,98. The most important variable for linear regression is year of house being built, which other 2 models almost don't look at. Another very suprising fact is, that the most important variables for XGBoost (`grade` and `latitude`) when shuffled improve linear regression loss!\n",
    "\n",
    "![reg](img/reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was very suprised at my findings so I plotted the linear regression coefficients. Interestingly, the most important variable (`year of built`) have coefficient equal almost zero. The variable `latitude` has high coefficient and high variable importance, but variable `waterfront` has also high coefficient, but variable importance under 1.\n",
    "\n",
    "![reg](img/reg_coef.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumamry\n",
    "\n",
    "Three of these models behaves differently. They look at different variables in theirs predictions, even though both XGBoost and Random Forest use similar technique of Decision Trees underneath. The analysis of linear regression model showed that one need to be carefull when using poor models and should always try to explain its predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
