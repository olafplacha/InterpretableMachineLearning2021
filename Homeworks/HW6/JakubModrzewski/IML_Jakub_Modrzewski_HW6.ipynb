{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IML_Jakub_Modrzewski_HW6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw8AZ5h7agml"
      },
      "source": [
        "# Fairness Metrics. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## TODO:\n",
        "\n",
        "  * For the selected data set, train at least one tree-based ensemble model (random forest, gbm, catboost or any other boosting)\n",
        "  * For a selected variable (protected attribute, if possible choose sex or race, but if you do not have such variable then choose any categorical varaible) calculate selected fairness metrics (at least 3, like statistical parity, • equal opportunity, predictive equality),\n",
        "  * train three or more candidate models (different variables, different transformations, different model structures) and compare fairness metrics between them.\n",
        "  * Comment on the results for points (2) and (3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEjfn4GJFdUm"
      },
      "source": [
        "## Data & preprocessing\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWO449BybHcy"
      },
      "source": [
        "For the sake of simplicity we'll re-use mushrooms dataset again, known already from the HW1, HW2 and HW3. Ordinnal encoder will be used, similarly to the HW3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t6oeNcRNRFI"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"mushrooms.csv\")\n",
        "encoder = OrdinalEncoder()\n",
        "\n",
        "y = df.loc[:,['class']]\n",
        "X = df.drop(['class'], axis=1)\n",
        "\n",
        "y_temp = encoder.fit_transform(y)\n",
        "X_temp = encoder.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_temp, y_temp, test_size=0.2, shuffle=True)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aQfjThAJpud"
      },
      "source": [
        "And we'll also need fairness metrics. Notation (based on the hiring environment for simplicity, as described in [here](https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb), though here we'll apply it to mushroom dataset):\n",
        "\n",
        "  * X ∈ Rᵈ: quantified features of the applicant(e.g. education, work experience, college GPA, etc.).\n",
        "  * A ∈ {0, 1}: a binary sensitive attribute(e.g. majority/minority).\n",
        "  * C :=c(X,A) ∈ {0,1}: binary predictor (e.g. hire/reject), which makes decision based on a score R:=r(x, a) ∈ [0,1].\n",
        "  * Y ∈ {0, 1}: target variable(e.g. if the candidate is truly capable of the position).\n",
        "  * We assume X, A, Y are generated from an underlying distribution D i.e. (X, A, Y) ~ D.\n",
        "  * We also denote P₀ [c] := P [c | A= 0]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQQE215xKV0E"
      },
      "source": [
        "# statistical parity\n",
        "# P₀ [C=1] - P₁ [C=1]\n",
        "\n",
        "def statisticalParity(data, predictions, protectedIndex, protectedValue):   \n",
        "   protectedClass = [(x,l) for (x,l) in zip(data, predictions) \n",
        "      if x[protectedIndex] == protectedValue]   \n",
        "   elseClass = [(x,l) for (x,l) in zip(data, predictions) \n",
        "      if x[protectedIndex] != protectedValue]\n",
        " \n",
        "   if len(protectedClass) == 0 or len(elseClass) == 0:\n",
        "      raise Exception(\"One of the classes is empty!\")\n",
        "   else:\n",
        "      protectedProb = sum(1 for (x,l) in protectedClass if l == 1) / len(protectedClass)\n",
        "      elseProb = sum(1 for (x,l) in elseClass  if l == 1) / len(elseClass)\n",
        " \n",
        "   return elseProb, protectedProb\n",
        "\n",
        "\n",
        "# equality of opportunity\n",
        "# P₀ [C = 1| Y = 1] - P₁ [C = 1| Y = 1]\n",
        "\n",
        "def equalOpportunity(data, predictions, labels, protectedIndex, protectedValue):   \n",
        "   protectedClass = [(x,p,l) for (x,p,l) in zip(data, predictions, labels) \n",
        "      if x[protectedIndex] == protectedValue and l == 1]   \n",
        "   elseClass = [(x,p,l) for (x,p,l) in zip(data, predictions, labels) \n",
        "      if x[protectedIndex] != protectedValue and l == 1]\n",
        " \n",
        "   if len(protectedClass) == 0 or len(elseClass) == 0:\n",
        "      raise Exception(\"One of the classes is empty!\")\n",
        "   else:\n",
        "      protectedProb = sum(1 for (x,p,l) in protectedClass if p == 1) / len(protectedClass)\n",
        "      elseProb = sum(1 for (x,p,l) in elseClass  if p == 1) / len(elseClass)\n",
        " \n",
        "   return elseProb, protectedProb\n",
        "\n",
        "# predictive rate parity - true/false\n",
        "# P₀ [Y = 1| C= 1] = P₁ [Y = 1| C= 1] / P₀ [Y = 0| C= 0] = P₁ [Y = 0| C= 0]\n",
        "\n",
        "def predictiveRateParity(data, predictions, labels, protectedIndex, protectedValue, predictionValue):   \n",
        "   protectedClass = [(x,p,l) for (x,p,l) in zip(data, predictions, labels) \n",
        "      if x[protectedIndex] == protectedValue and p == predictionValue]   \n",
        "   elseClass = [(x,p,l) for (x,p,l) in zip(data, predictions, labels) \n",
        "      if x[protectedIndex] != protectedValue and p == predictionValue]\n",
        " \n",
        "   if len(protectedClass) == 0 or len(elseClass) == 0:\n",
        "      raise Exception(\"One of the classes is empty!\")\n",
        "   else:\n",
        "      protectedProb = sum(1 for (x,p,l) in protectedClass if l == predictionValue) / len(protectedClass)\n",
        "      elseProb = sum(1 for (x,p,l) in elseClass  if l == predictionValue) / len(elseClass)\n",
        " \n",
        "   return elseProb, protectedProb\n",
        "\n",
        "def calculateMetrics(data, predictions, labels, protectedIndex, protectedValue):\n",
        "   s_par = statisticalParity(data, predictions, protectedIndex, protectedValue)\n",
        "   e_opp = equalOpportunity(data, predictions, labels, protectedIndex, protectedValue)\n",
        "   p_r_par_true = predictiveRateParity(data, predictions, labels, protectedIndex, protectedValue, 1)\n",
        "   p_r_par_false = predictiveRateParity(data, predictions, labels, protectedIndex, protectedValue, 0)\n",
        "   print(f\"Protected probability and complement probability (statistical parity): {s_par}\")\n",
        "   print(f\"Protected probability and complement probability (equal opportunity): {e_opp}\")\n",
        "   print(f\"Protected probability and complement probability (predictive rate parity - true): {p_r_par_true}\")\n",
        "   print(f\"Protected probability and complement probability (predictive rate parity - false): {p_r_par_false}\")"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfzQ4PuYFtKf"
      },
      "source": [
        "## First model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvTXLaGJEnDg"
      },
      "source": [
        "For the tree-based model, let's take xgboost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMujv7XkDe7H",
        "outputId": "59242ca4-f24a-46a3-945e-be8b9e70ad95"
      },
      "source": [
        "import xgboost\n",
        "\n",
        "xgb = xgboost.XGBClassifier().fit(X_train, y_train)\n",
        "print(f\"\\nModel score on test set: {xgb.score(X_test, y_test)}\")\n",
        "print(f\"Predictions for the first 10 observations: {xgb.predict(X_test[:10])}\")\n",
        "print(f\"True values for the first 10 observations: {y_test[:10].tolist()}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model score on test set: 1.0\n",
            "Predictions for the first 10 observations: [1. 0. 0. 0. 1. 1. 0. 1. 1. 0.]\n",
            "True values for the first 10 observations: [[1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3ScK6PVEf7c"
      },
      "source": [
        "Since mushroom dataset is fairly simple, our xboost model trained very well. Let's calculate fairness metrics and see how they behave."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71RY8YiG-uLJ",
        "outputId": "59455bc2-61a2-423a-a5b9-ec6ddc7a5edf"
      },
      "source": [
        "calculateMetrics(X_test, xgb.predict(X_test), y_test, 9, 0)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Protected probability and complement probability (statistical parity): (0.42185792349726775, 0.5957746478873239)\n",
            "Protected probability and complement probability (equal opportunity): (1.0, 1.0)\n",
            "Protected probability and complement probability (predictive rate parity - true): (1.0, 1.0)\n",
            "Protected probability and complement probability (predictive rate parity - false): (1.0, 1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awwYCOWPFUTO"
      },
      "source": [
        "## Second model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSX2KhHAAWUB"
      },
      "source": [
        "Next two models appeared in one of the previous homeworks. The idea was to try overly simple and overly complicated DNN architectures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XGs4r0oT_lb",
        "outputId": "af3668b5-bf11-4c3a-80dc-f9d0d1ffdcae"
      },
      "source": [
        "# And now, for the last point, let's re-use some code from HW1 to train simple NN classifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid')) \n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=16, epochs=100, validation_data=(X_val, y_val))\n",
        "\n",
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "325/325 [==============================] - 1s 2ms/step - loss: 1.1691 - accuracy: 0.5588 - val_loss: 1.0178 - val_accuracy: 0.5508\n",
            "Epoch 2/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 1.0331 - accuracy: 0.5476 - val_loss: 0.9457 - val_accuracy: 0.5531\n",
            "Epoch 3/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.9742 - accuracy: 0.5636 - val_loss: 0.8995 - val_accuracy: 0.5769\n",
            "Epoch 4/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.9299 - accuracy: 0.5863 - val_loss: 0.8601 - val_accuracy: 0.5969\n",
            "Epoch 5/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.8911 - accuracy: 0.6134 - val_loss: 0.8249 - val_accuracy: 0.6238\n",
            "Epoch 6/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.8556 - accuracy: 0.6363 - val_loss: 0.7919 - val_accuracy: 0.6362\n",
            "Epoch 7/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.8228 - accuracy: 0.6545 - val_loss: 0.7625 - val_accuracy: 0.6531\n",
            "Epoch 8/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.7925 - accuracy: 0.6672 - val_loss: 0.7342 - val_accuracy: 0.6731\n",
            "Epoch 9/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.7641 - accuracy: 0.6811 - val_loss: 0.7082 - val_accuracy: 0.6838\n",
            "Epoch 10/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.7373 - accuracy: 0.6909 - val_loss: 0.6832 - val_accuracy: 0.6938\n",
            "Epoch 11/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.7120 - accuracy: 0.7048 - val_loss: 0.6600 - val_accuracy: 0.7054\n",
            "Epoch 12/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.6877 - accuracy: 0.7105 - val_loss: 0.6375 - val_accuracy: 0.7108\n",
            "Epoch 13/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.6645 - accuracy: 0.7180 - val_loss: 0.6159 - val_accuracy: 0.7208\n",
            "Epoch 14/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.6426 - accuracy: 0.7265 - val_loss: 0.5957 - val_accuracy: 0.7262\n",
            "Epoch 15/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.6217 - accuracy: 0.7319 - val_loss: 0.5765 - val_accuracy: 0.7369\n",
            "Epoch 16/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.6017 - accuracy: 0.7380 - val_loss: 0.5583 - val_accuracy: 0.7485\n",
            "Epoch 17/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.5827 - accuracy: 0.7413 - val_loss: 0.5408 - val_accuracy: 0.7515\n",
            "Epoch 18/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.5645 - accuracy: 0.7496 - val_loss: 0.5242 - val_accuracy: 0.7585\n",
            "Epoch 19/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.5472 - accuracy: 0.7538 - val_loss: 0.5082 - val_accuracy: 0.7654\n",
            "Epoch 20/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.5308 - accuracy: 0.7607 - val_loss: 0.4935 - val_accuracy: 0.7685\n",
            "Epoch 21/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.5151 - accuracy: 0.7692 - val_loss: 0.4793 - val_accuracy: 0.7754\n",
            "Epoch 22/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.5001 - accuracy: 0.7719 - val_loss: 0.4652 - val_accuracy: 0.7823\n",
            "Epoch 23/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.4859 - accuracy: 0.7794 - val_loss: 0.4524 - val_accuracy: 0.7885\n",
            "Epoch 24/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.4725 - accuracy: 0.7836 - val_loss: 0.4402 - val_accuracy: 0.7969\n",
            "Epoch 25/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.4598 - accuracy: 0.7878 - val_loss: 0.4289 - val_accuracy: 0.8015\n",
            "Epoch 26/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.4481 - accuracy: 0.7932 - val_loss: 0.4182 - val_accuracy: 0.8100\n",
            "Epoch 27/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.4369 - accuracy: 0.7998 - val_loss: 0.4081 - val_accuracy: 0.8146\n",
            "Epoch 28/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.4265 - accuracy: 0.8055 - val_loss: 0.3986 - val_accuracy: 0.8254\n",
            "Epoch 29/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.4168 - accuracy: 0.8111 - val_loss: 0.3896 - val_accuracy: 0.8338\n",
            "Epoch 30/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.4077 - accuracy: 0.8173 - val_loss: 0.3811 - val_accuracy: 0.8392\n",
            "Epoch 31/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3992 - accuracy: 0.8223 - val_loss: 0.3737 - val_accuracy: 0.8469\n",
            "Epoch 32/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3912 - accuracy: 0.8282 - val_loss: 0.3663 - val_accuracy: 0.8500\n",
            "Epoch 33/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3837 - accuracy: 0.8317 - val_loss: 0.3592 - val_accuracy: 0.8562\n",
            "Epoch 34/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3766 - accuracy: 0.8386 - val_loss: 0.3527 - val_accuracy: 0.8662\n",
            "Epoch 35/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3701 - accuracy: 0.8457 - val_loss: 0.3468 - val_accuracy: 0.8685\n",
            "Epoch 36/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3639 - accuracy: 0.8467 - val_loss: 0.3411 - val_accuracy: 0.8708\n",
            "Epoch 37/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3583 - accuracy: 0.8513 - val_loss: 0.3358 - val_accuracy: 0.8731\n",
            "Epoch 38/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3530 - accuracy: 0.8555 - val_loss: 0.3309 - val_accuracy: 0.8754\n",
            "Epoch 39/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3480 - accuracy: 0.8592 - val_loss: 0.3262 - val_accuracy: 0.8777\n",
            "Epoch 40/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3434 - accuracy: 0.8636 - val_loss: 0.3222 - val_accuracy: 0.8823\n",
            "Epoch 41/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3391 - accuracy: 0.8675 - val_loss: 0.3181 - val_accuracy: 0.8846\n",
            "Epoch 42/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.8694 - val_loss: 0.3143 - val_accuracy: 0.8885\n",
            "Epoch 43/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3315 - accuracy: 0.8734 - val_loss: 0.3108 - val_accuracy: 0.8885\n",
            "Epoch 44/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8757 - val_loss: 0.3076 - val_accuracy: 0.8923\n",
            "Epoch 45/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3249 - accuracy: 0.8782 - val_loss: 0.3046 - val_accuracy: 0.8954\n",
            "Epoch 46/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8796 - val_loss: 0.3016 - val_accuracy: 0.8954\n",
            "Epoch 47/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3190 - accuracy: 0.8815 - val_loss: 0.2991 - val_accuracy: 0.9000\n",
            "Epoch 48/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3162 - accuracy: 0.8834 - val_loss: 0.2963 - val_accuracy: 0.9031\n",
            "Epoch 49/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3136 - accuracy: 0.8852 - val_loss: 0.2940 - val_accuracy: 0.9054\n",
            "Epoch 50/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3113 - accuracy: 0.8869 - val_loss: 0.2916 - val_accuracy: 0.9077\n",
            "Epoch 51/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3091 - accuracy: 0.8871 - val_loss: 0.2896 - val_accuracy: 0.9069\n",
            "Epoch 52/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3068 - accuracy: 0.8882 - val_loss: 0.2872 - val_accuracy: 0.9100\n",
            "Epoch 53/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3048 - accuracy: 0.8902 - val_loss: 0.2853 - val_accuracy: 0.9108\n",
            "Epoch 54/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3029 - accuracy: 0.8911 - val_loss: 0.2834 - val_accuracy: 0.9115\n",
            "Epoch 55/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.3010 - accuracy: 0.8919 - val_loss: 0.2816 - val_accuracy: 0.9115\n",
            "Epoch 56/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2993 - accuracy: 0.8940 - val_loss: 0.2801 - val_accuracy: 0.9131\n",
            "Epoch 57/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2977 - accuracy: 0.8936 - val_loss: 0.2784 - val_accuracy: 0.9138\n",
            "Epoch 58/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2961 - accuracy: 0.8957 - val_loss: 0.2769 - val_accuracy: 0.9162\n",
            "Epoch 59/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2945 - accuracy: 0.8963 - val_loss: 0.2752 - val_accuracy: 0.9154\n",
            "Epoch 60/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2932 - accuracy: 0.8967 - val_loss: 0.2739 - val_accuracy: 0.9169\n",
            "Epoch 61/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2918 - accuracy: 0.8971 - val_loss: 0.2725 - val_accuracy: 0.9169\n",
            "Epoch 62/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2905 - accuracy: 0.8982 - val_loss: 0.2711 - val_accuracy: 0.9177\n",
            "Epoch 63/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2892 - accuracy: 0.8992 - val_loss: 0.2699 - val_accuracy: 0.9169\n",
            "Epoch 64/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2879 - accuracy: 0.8992 - val_loss: 0.2684 - val_accuracy: 0.9185\n",
            "Epoch 65/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2868 - accuracy: 0.9004 - val_loss: 0.2673 - val_accuracy: 0.9185\n",
            "Epoch 66/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2857 - accuracy: 0.9002 - val_loss: 0.2663 - val_accuracy: 0.9177\n",
            "Epoch 67/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2847 - accuracy: 0.9006 - val_loss: 0.2650 - val_accuracy: 0.9185\n",
            "Epoch 68/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2836 - accuracy: 0.9009 - val_loss: 0.2639 - val_accuracy: 0.9192\n",
            "Epoch 69/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2826 - accuracy: 0.9023 - val_loss: 0.2628 - val_accuracy: 0.9169\n",
            "Epoch 70/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2816 - accuracy: 0.9019 - val_loss: 0.2619 - val_accuracy: 0.9192\n",
            "Epoch 71/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2807 - accuracy: 0.9021 - val_loss: 0.2609 - val_accuracy: 0.9192\n",
            "Epoch 72/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2798 - accuracy: 0.9034 - val_loss: 0.2600 - val_accuracy: 0.9192\n",
            "Epoch 73/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2788 - accuracy: 0.9040 - val_loss: 0.2593 - val_accuracy: 0.9192\n",
            "Epoch 74/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2781 - accuracy: 0.9042 - val_loss: 0.2583 - val_accuracy: 0.9200\n",
            "Epoch 75/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2773 - accuracy: 0.9033 - val_loss: 0.2572 - val_accuracy: 0.9185\n",
            "Epoch 76/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2764 - accuracy: 0.9042 - val_loss: 0.2563 - val_accuracy: 0.9177\n",
            "Epoch 77/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2757 - accuracy: 0.9050 - val_loss: 0.2557 - val_accuracy: 0.9192\n",
            "Epoch 78/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2749 - accuracy: 0.9046 - val_loss: 0.2549 - val_accuracy: 0.9192\n",
            "Epoch 79/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2742 - accuracy: 0.9046 - val_loss: 0.2542 - val_accuracy: 0.9192\n",
            "Epoch 80/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2736 - accuracy: 0.9052 - val_loss: 0.2534 - val_accuracy: 0.9200\n",
            "Epoch 81/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2728 - accuracy: 0.9058 - val_loss: 0.2525 - val_accuracy: 0.9208\n",
            "Epoch 82/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2721 - accuracy: 0.9065 - val_loss: 0.2519 - val_accuracy: 0.9200\n",
            "Epoch 83/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2714 - accuracy: 0.9063 - val_loss: 0.2510 - val_accuracy: 0.9215\n",
            "Epoch 84/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2708 - accuracy: 0.9077 - val_loss: 0.2503 - val_accuracy: 0.9208\n",
            "Epoch 85/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2701 - accuracy: 0.9075 - val_loss: 0.2497 - val_accuracy: 0.9215\n",
            "Epoch 86/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2695 - accuracy: 0.9083 - val_loss: 0.2489 - val_accuracy: 0.9215\n",
            "Epoch 87/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2689 - accuracy: 0.9086 - val_loss: 0.2483 - val_accuracy: 0.9215\n",
            "Epoch 88/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2683 - accuracy: 0.9086 - val_loss: 0.2477 - val_accuracy: 0.9215\n",
            "Epoch 89/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2676 - accuracy: 0.9094 - val_loss: 0.2472 - val_accuracy: 0.9223\n",
            "Epoch 90/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2671 - accuracy: 0.9090 - val_loss: 0.2464 - val_accuracy: 0.9223\n",
            "Epoch 91/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2665 - accuracy: 0.9096 - val_loss: 0.2460 - val_accuracy: 0.9223\n",
            "Epoch 92/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2660 - accuracy: 0.9109 - val_loss: 0.2450 - val_accuracy: 0.9215\n",
            "Epoch 93/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2654 - accuracy: 0.9106 - val_loss: 0.2445 - val_accuracy: 0.9215\n",
            "Epoch 94/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2649 - accuracy: 0.9108 - val_loss: 0.2439 - val_accuracy: 0.9215\n",
            "Epoch 95/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2643 - accuracy: 0.9106 - val_loss: 0.2433 - val_accuracy: 0.9223\n",
            "Epoch 96/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2637 - accuracy: 0.9115 - val_loss: 0.2425 - val_accuracy: 0.9200\n",
            "Epoch 97/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2632 - accuracy: 0.9111 - val_loss: 0.2421 - val_accuracy: 0.9223\n",
            "Epoch 98/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2627 - accuracy: 0.9111 - val_loss: 0.2418 - val_accuracy: 0.9208\n",
            "Epoch 99/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2622 - accuracy: 0.9115 - val_loss: 0.2411 - val_accuracy: 0.9215\n",
            "Epoch 100/100\n",
            "325/325 [==============================] - 0s 1ms/step - loss: 0.2618 - accuracy: 0.9115 - val_loss: 0.2407 - val_accuracy: 0.9215\n",
            "51/51 [==============================] - 0s 1ms/step - loss: 0.2388 - accuracy: 0.9231\n",
            "Test loss: 0.23880435526371002\n",
            "Test accuracy: 0.9230769276618958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2zq8UIwAlYf"
      },
      "source": [
        "Fairness metrics for the second model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j60HpgCD9DlS",
        "outputId": "6c09adba-2696-4b69-cea6-170661024166"
      },
      "source": [
        "calculateMetrics(X_test, (model.predict(X_test) > 0.5).astype(int), y_test, 9, 0)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Protected probability and complement probability (statistical parity): (0.3901639344262295, 0.5422535211267606)\n",
            "Protected probability and complement probability (equal opportunity): (0.9248704663212435, 0.8416075650118203)\n",
            "Protected probability and complement probability (predictive rate parity - true): (1.0, 0.9246753246753247)\n",
            "Protected probability and complement probability (predictive rate parity - false): (0.9480286738351255, 0.7938461538461539)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcd0wy7mZHpX"
      },
      "source": [
        "## Third model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R18T5oPSj2bI",
        "outputId": "bf0e414d-50ed-49da-8588-d50ebe775b55"
      },
      "source": [
        "# And now, for the last point, let's re-use some code from HW1 to train simple NN classifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model_2 = Sequential()\n",
        "model_2.add(Dense(256, activation='relu'))\n",
        "model_2.add(Dropout(rate=0.25))\n",
        "model_2.add(Dense(128, activation='relu'))\n",
        "model_2.add(Dropout(rate=0.25))\n",
        "model_2.add(Dense(64, activation='relu'))\n",
        "model_2.add(Dropout(rate=0.25))\n",
        "model_2.add(Dense(32, activation='relu'))\n",
        "model_2.add(Dropout(rate=0.25))\n",
        "model_2.add(Dense(1, activation='sigmoid')) \n",
        "\n",
        "model_2.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "history = model_2.fit(X_train, y_train, batch_size=16, epochs=30, validation_data=(X_val, y_val))\n",
        "\n",
        "score = model_2.evaluate(X_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "325/325 [==============================] - 2s 3ms/step - loss: 0.5944 - accuracy: 0.6726 - val_loss: 0.4213 - val_accuracy: 0.8169\n",
            "Epoch 2/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.4431 - accuracy: 0.8007 - val_loss: 0.3037 - val_accuracy: 0.8900\n",
            "Epoch 3/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.3413 - accuracy: 0.8615 - val_loss: 0.2256 - val_accuracy: 0.9254\n",
            "Epoch 4/30\n",
            "325/325 [==============================] - 1s 2ms/step - loss: 0.2927 - accuracy: 0.8886 - val_loss: 0.1808 - val_accuracy: 0.9377\n",
            "Epoch 5/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.2493 - accuracy: 0.9023 - val_loss: 0.1593 - val_accuracy: 0.9415\n",
            "Epoch 6/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.2120 - accuracy: 0.9154 - val_loss: 0.1215 - val_accuracy: 0.9538\n",
            "Epoch 7/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.1744 - accuracy: 0.9334 - val_loss: 0.0931 - val_accuracy: 0.9677\n",
            "Epoch 8/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.1459 - accuracy: 0.9442 - val_loss: 0.0666 - val_accuracy: 0.9800\n",
            "Epoch 9/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.1250 - accuracy: 0.9554 - val_loss: 0.0535 - val_accuracy: 0.9877\n",
            "Epoch 10/30\n",
            "325/325 [==============================] - 1s 2ms/step - loss: 0.1023 - accuracy: 0.9638 - val_loss: 0.0348 - val_accuracy: 0.9915\n",
            "Epoch 11/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0900 - accuracy: 0.9702 - val_loss: 0.0274 - val_accuracy: 0.9931\n",
            "Epoch 12/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0753 - accuracy: 0.9725 - val_loss: 0.0215 - val_accuracy: 0.9938\n",
            "Epoch 13/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0615 - accuracy: 0.9806 - val_loss: 0.0146 - val_accuracy: 0.9962\n",
            "Epoch 14/30\n",
            "325/325 [==============================] - 1s 2ms/step - loss: 0.0427 - accuracy: 0.9862 - val_loss: 0.0090 - val_accuracy: 0.9985\n",
            "Epoch 15/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0467 - accuracy: 0.9833 - val_loss: 0.0084 - val_accuracy: 0.9992\n",
            "Epoch 16/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0436 - accuracy: 0.9863 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
            "Epoch 17/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0350 - accuracy: 0.9885 - val_loss: 0.0051 - val_accuracy: 0.9992\n",
            "Epoch 18/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0291 - accuracy: 0.9919 - val_loss: 0.0049 - val_accuracy: 0.9992\n",
            "Epoch 19/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0278 - accuracy: 0.9912 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 20/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9929 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 21/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9935 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 22/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0210 - accuracy: 0.9933 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 23/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0159 - accuracy: 0.9938 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 24/30\n",
            "325/325 [==============================] - 1s 2ms/step - loss: 0.0137 - accuracy: 0.9965 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 25/30\n",
            "325/325 [==============================] - 1s 2ms/step - loss: 0.0133 - accuracy: 0.9965 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "Epoch 26/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0142 - accuracy: 0.9942 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 27/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0139 - accuracy: 0.9956 - val_loss: 9.8022e-04 - val_accuracy: 1.0000\n",
            "Epoch 28/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0090 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
            "Epoch 29/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0119 - accuracy: 0.9954 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 30/30\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 0.0065 - accuracy: 0.9987 - val_loss: 4.6375e-04 - val_accuracy: 1.0000\n",
            "51/51 [==============================] - 0s 1ms/step - loss: 3.5656e-04 - accuracy: 1.0000\n",
            "Test loss: 0.00035655719693750143\n",
            "Test accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W_ObTzTAsfb"
      },
      "source": [
        "And fairness metrics for the third model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Edu70x6i_9Z_",
        "outputId": "f7c9717b-b318-4b56-a380-01f0ca0311ed"
      },
      "source": [
        "calculateMetrics(X_test, (model_2.predict(X_test) > 0.5).astype(int), y_test, 9, 0)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Protected probability and complement probability (statistical parity): (0.42185792349726775, 0.5957746478873239)\n",
            "Protected probability and complement probability (equal opportunity): (1.0, 1.0)\n",
            "Protected probability and complement probability (predictive rate parity - true): (1.0, 1.0)\n",
            "Protected probability and complement probability (predictive rate parity - false): (1.0, 1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kMieK2HAybu"
      },
      "source": [
        "## Comment\n",
        "\n",
        "---\n",
        "\n",
        "Although statistical parity shown some variation among all models, both XGB and overly-complicated DNN had a perfect equal opportunity and predictive rate parity. It might've been caused by the simplicity of our dataset - these 2 models reached 100% accuracy. Interrestingly, lower prediction confidence of simple-DNN led to worse fairness metrics.\n",
        "\n",
        "As mushroom dataset doesn't have any natural candidates for protected values, I chose feature #9 - it had decently balanced distributioin of 0's vs other values."
      ]
    }
  ]
}